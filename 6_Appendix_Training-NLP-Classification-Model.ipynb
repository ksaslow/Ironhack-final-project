{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP CLASSIFICATION MODEL\n",
    "### TRAIN MODEL TO PREDICT PROBABILITY OF POSITIVE/NEGATIVE TWEET BASED ON LOCATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0:\n",
    "### Importing libraries and data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TweetText</th>\n",
       "      <th>Handle</th>\n",
       "      <th>Location</th>\n",
       "      <th>Country</th>\n",
       "      <th>Language</th>\n",
       "      <th>text_processed</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @rachel_groves: Daily goals: Wake up early....</td>\n",
       "      <td>joscchh</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>en</td>\n",
       "      <td>['daili', 'goal', 'wake', 'earli', 'drink', 'c...</td>\n",
       "      <td>0.194048</td>\n",
       "      <td>0.427381</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @EurAsiaBridgeEU: good topic - well done.</td>\n",
       "      <td>joscchh</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>en</td>\n",
       "      <td>['good', 'topic', 'well', 'done']</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@jpbrice Life is always precious - every singl...</td>\n",
       "      <td>joscchh</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>en</td>\n",
       "      <td>['jpbrice', 'life', 'alway', 'precious', 'ever...</td>\n",
       "      <td>0.272381</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@PeterShepherd8 @MYorke27 @ZEReadyOregon Great...</td>\n",
       "      <td>joscchh</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>en</td>\n",
       "      <td>['petershepherd', 'myork', 'zereadyoregon', 'g...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.385417</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @jpbrice: “You never know how strong you ar...</td>\n",
       "      <td>joscchh</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>en</td>\n",
       "      <td>['never', 'know', 'strong', 'strong', 'onli', ...</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           TweetText   Handle  \\\n",
       "0  RT @rachel_groves: Daily goals: Wake up early....  joscchh   \n",
       "1       RT @EurAsiaBridgeEU: good topic - well done.  joscchh   \n",
       "2  @jpbrice Life is always precious - every singl...  joscchh   \n",
       "3  @PeterShepherd8 @MYorke27 @ZEReadyOregon Great...  joscchh   \n",
       "4  RT @jpbrice: “You never know how strong you ar...  joscchh   \n",
       "\n",
       "            Location  Country Language  \\\n",
       "0  Bonn, Deutschland  Germany       en   \n",
       "1  Bonn, Deutschland  Germany       en   \n",
       "2  Bonn, Deutschland  Germany       en   \n",
       "3  Bonn, Deutschland  Germany       en   \n",
       "4  Bonn, Deutschland  Germany       en   \n",
       "\n",
       "                                      text_processed  Polarity  Subjectivity  \\\n",
       "0  ['daili', 'goal', 'wake', 'earli', 'drink', 'c...  0.194048      0.427381   \n",
       "1                  ['good', 'topic', 'well', 'done']  0.700000      0.600000   \n",
       "2  ['jpbrice', 'life', 'alway', 'precious', 'ever...  0.272381      0.609524   \n",
       "3  ['petershepherd', 'myork', 'zereadyoregon', 'g...  0.200000      0.385417   \n",
       "4  ['never', 'know', 'strong', 'strong', 'onli', ...  0.288889      0.822222   \n",
       "\n",
       "  Sentiment  \n",
       "0  positive  \n",
       "1  positive  \n",
       "2  positive  \n",
       "3  positive  \n",
       "4  positive  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use dataframe with tweets that have gone through pre-processing for NLP, and which are already tagged after TextBlob sentiment_object:\n",
    "\n",
    "tweets_analysis = pd.read_csv('tweets_analysis.csv')\n",
    "tweets_analysis.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "tweets_analysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tweets_analysis['text_processed'][0])\n",
    "\n",
    "# SHOULD BE a list. Thats how it was exported after NLP-pre-processing steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       ['daili', 'goal', 'wake', 'earli', 'drink', 'c...\n",
       "1                       ['good', 'topic', 'well', 'done']\n",
       "2       ['jpbrice', 'life', 'alway', 'precious', 'ever...\n",
       "3       ['petershepherd', 'myork', 'zereadyoregon', 'g...\n",
       "4       ['never', 'know', 'strong', 'strong', 'onli', ...\n",
       "                              ...                        \n",
       "7854    ['friedrich', 'merz', 'frau', 'geheiratet', 'm...\n",
       "7855    ['hatt', 'nie', 'gedacht', 'mal', 'sag', 'druc...\n",
       "7856    ['echt', 'frau', 'job', 'abgibt', 'au', 'grund...\n",
       "7857    ['perfid', 'begriff', 'neidsteu', 'uberhaupt',...\n",
       "7858    ['gerad', 'foto', 'gepostet', 'berlin', 'germa...\n",
       "Name: text_processed, Length: 7859, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_analysis['text_processed']\n",
    "# can I just use regex to remove the '' around each token?? Or will that not be enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = tweets_analysis['text_processed'][0]\n",
    "type(test)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1:\n",
    "### DF processed text format got messed up in import/export. Re-do preprocessing steps before training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_analysis['text'] = tweets_analysis['TweetText']\n",
    "\n",
    "def clean_up(x):\n",
    "    \"\"\" Clean up the text before NLP. \n",
    "    Remove Punctuation, remove retweet and @.\n",
    "    Keep German characters as vowels.\"\"\"\n",
    "    \n",
    "    remove_rt = lambda x: re.sub('RT @\\w+: ', \"\", x)\n",
    "    lab = lambda x: re.sub('[^äöüÄÖÜA-Za-z ]+','', x)\n",
    "    tw_1 = remove_rt(x)\n",
    "    tw_2 = lab(tw_1)\n",
    "    tw_3 =  tw_2.lower()\n",
    "    \n",
    "    return tw_3\n",
    "\n",
    "\n",
    "tweets_analysis['text'] = tweets_analysis['text'].apply(clean_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    \"\"\"Tokenize the tweets.\"\"\"\n",
    "    \n",
    "    from nltk.tokenize import word_tokenize\n",
    "    tokenized_x = word_tokenize(x)\n",
    "    return tokenized_x\n",
    "\n",
    "\n",
    "tweets_analysis['tokenized_tw'] = tweets_analysis['text'].apply(tokenize)\n",
    "# tweets_analysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_en = tweets_analysis[tweets_analysis['Language']=='en']\n",
    "tweets_de = tweets_analysis[tweets_analysis['Language']=='de']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_lemmatize_en(l):\n",
    "    \"\"\"This function performs both stemming and lemmatization on our english tweets.\"\"\"\n",
    "    \n",
    "    \n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    l = [lemmatizer.lemmatize(item) for item in l]\n",
    "    from nltk.stem import SnowballStemmer\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    l = [stemmer.stem(item) for item in l]\n",
    "    \n",
    "    return l \n",
    "\n",
    "\n",
    "tweets_en['stem_lemm_tw'] = tweets_en['tokenized_tw'].apply(stem_lemmatize_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_lemmatize_de(l):\n",
    "    \"\"\"This function performs both stemming and lemmatization on our german tweets.\"\"\"\n",
    "    \n",
    "    \n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    l = [lemmatizer.lemmatize(item) for item in l]\n",
    "    from nltk.stem import SnowballStemmer\n",
    "    stemmer = SnowballStemmer(\"german\")\n",
    "    l = [stemmer.stem(item) for item in l]\n",
    "    \n",
    "    return l \n",
    "\n",
    "tweets_de['stem_lemm_tw'] = tweets_de['tokenized_tw'].apply(stem_lemmatize_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_en(l):\n",
    "    \"\"\"Remove english stopwords from a list of tweets.\"\"\"\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "    stopwords_list = stopwords.words(\"english\")\n",
    "    l = [item for item in l if not item in stopwords_list]\n",
    "    \n",
    "    return l\n",
    "\n",
    "tweets_en['remove_stopwords_tw'] = tweets_en['stem_lemm_tw'].apply(remove_stopwords_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_de(l):\n",
    "    \"\"\"Remove german stopwords from a list of tweets.\"\"\"\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "    stopwords_list = stopwords.words(\"german\")\n",
    "    l = [item for item in l if not item in stopwords_list]\n",
    "    \n",
    "    return l\n",
    "\n",
    "tweets_de['remove_stopwords_tw'] = tweets_de['stem_lemm_tw'].apply(remove_stopwords_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TweetText</th>\n",
       "      <th>Handle</th>\n",
       "      <th>Location</th>\n",
       "      <th>Country</th>\n",
       "      <th>Language</th>\n",
       "      <th>text_processed</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_tw</th>\n",
       "      <th>stem_lemm_tw</th>\n",
       "      <th>remove_stopwords_tw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @rachel_groves: Daily goals: Wake up early....</td>\n",
       "      <td>joscchh</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>en</td>\n",
       "      <td>['daili', 'goal', 'wake', 'earli', 'drink', 'c...</td>\n",
       "      <td>0.194048</td>\n",
       "      <td>0.427381</td>\n",
       "      <td>positive</td>\n",
       "      <td>daily goals wake up early drink coffee work ha...</td>\n",
       "      <td>[daily, goals, wake, up, early, drink, coffee,...</td>\n",
       "      <td>[daili, goal, wake, up, earli, drink, coffe, w...</td>\n",
       "      <td>[daili, goal, wake, earli, drink, coffe, work,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @EurAsiaBridgeEU: good topic - well done.</td>\n",
       "      <td>joscchh</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>en</td>\n",
       "      <td>['good', 'topic', 'well', 'done']</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>positive</td>\n",
       "      <td>good topic  well done</td>\n",
       "      <td>[good, topic, well, done]</td>\n",
       "      <td>[good, topic, well, done]</td>\n",
       "      <td>[good, topic, well, done]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@jpbrice Life is always precious - every singl...</td>\n",
       "      <td>joscchh</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>en</td>\n",
       "      <td>['jpbrice', 'life', 'alway', 'precious', 'ever...</td>\n",
       "      <td>0.272381</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>positive</td>\n",
       "      <td>jpbrice life is always precious  every single ...</td>\n",
       "      <td>[jpbrice, life, is, always, precious, every, s...</td>\n",
       "      <td>[jpbrice, life, is, alway, precious, everi, si...</td>\n",
       "      <td>[jpbrice, life, alway, precious, everi, singl,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@PeterShepherd8 @MYorke27 @ZEReadyOregon Great...</td>\n",
       "      <td>joscchh</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>en</td>\n",
       "      <td>['petershepherd', 'myork', 'zereadyoregon', 'g...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.385417</td>\n",
       "      <td>positive</td>\n",
       "      <td>petershepherd myorke zereadyoregon great  we h...</td>\n",
       "      <td>[petershepherd, myorke, zereadyoregon, great, ...</td>\n",
       "      <td>[petershepherd, myork, zereadyoregon, great, w...</td>\n",
       "      <td>[petershepherd, myork, zereadyoregon, great, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @jpbrice: “You never know how strong you ar...</td>\n",
       "      <td>joscchh</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>en</td>\n",
       "      <td>['never', 'know', 'strong', 'strong', 'onli', ...</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>positive</td>\n",
       "      <td>you never know how strong you are until being ...</td>\n",
       "      <td>[you, never, know, how, strong, you, are, unti...</td>\n",
       "      <td>[you, never, know, how, strong, you, are, unti...</td>\n",
       "      <td>[never, know, strong, strong, onli, choic, quo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           TweetText   Handle  \\\n",
       "0  RT @rachel_groves: Daily goals: Wake up early....  joscchh   \n",
       "1       RT @EurAsiaBridgeEU: good topic - well done.  joscchh   \n",
       "2  @jpbrice Life is always precious - every singl...  joscchh   \n",
       "3  @PeterShepherd8 @MYorke27 @ZEReadyOregon Great...  joscchh   \n",
       "4  RT @jpbrice: “You never know how strong you ar...  joscchh   \n",
       "\n",
       "            Location  Country Language  \\\n",
       "0  Bonn, Deutschland  Germany       en   \n",
       "1  Bonn, Deutschland  Germany       en   \n",
       "2  Bonn, Deutschland  Germany       en   \n",
       "3  Bonn, Deutschland  Germany       en   \n",
       "4  Bonn, Deutschland  Germany       en   \n",
       "\n",
       "                                      text_processed  Polarity  Subjectivity  \\\n",
       "0  ['daili', 'goal', 'wake', 'earli', 'drink', 'c...  0.194048      0.427381   \n",
       "1                  ['good', 'topic', 'well', 'done']  0.700000      0.600000   \n",
       "2  ['jpbrice', 'life', 'alway', 'precious', 'ever...  0.272381      0.609524   \n",
       "3  ['petershepherd', 'myork', 'zereadyoregon', 'g...  0.200000      0.385417   \n",
       "4  ['never', 'know', 'strong', 'strong', 'onli', ...  0.288889      0.822222   \n",
       "\n",
       "  Sentiment                                               text  \\\n",
       "0  positive  daily goals wake up early drink coffee work ha...   \n",
       "1  positive                              good topic  well done   \n",
       "2  positive  jpbrice life is always precious  every single ...   \n",
       "3  positive  petershepherd myorke zereadyoregon great  we h...   \n",
       "4  positive  you never know how strong you are until being ...   \n",
       "\n",
       "                                        tokenized_tw  \\\n",
       "0  [daily, goals, wake, up, early, drink, coffee,...   \n",
       "1                          [good, topic, well, done]   \n",
       "2  [jpbrice, life, is, always, precious, every, s...   \n",
       "3  [petershepherd, myorke, zereadyoregon, great, ...   \n",
       "4  [you, never, know, how, strong, you, are, unti...   \n",
       "\n",
       "                                        stem_lemm_tw  \\\n",
       "0  [daili, goal, wake, up, earli, drink, coffe, w...   \n",
       "1                          [good, topic, well, done]   \n",
       "2  [jpbrice, life, is, alway, precious, everi, si...   \n",
       "3  [petershepherd, myork, zereadyoregon, great, w...   \n",
       "4  [you, never, know, how, strong, you, are, unti...   \n",
       "\n",
       "                                 remove_stopwords_tw  \n",
       "0  [daili, goal, wake, earli, drink, coffe, work,...  \n",
       "1                          [good, topic, well, done]  \n",
       "2  [jpbrice, life, alway, precious, everi, singl,...  \n",
       "3  [petershepherd, myork, zereadyoregon, great, l...  \n",
       "4  [never, know, strong, strong, onli, choic, quo...  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TweetText</th>\n",
       "      <th>Handle</th>\n",
       "      <th>Location</th>\n",
       "      <th>Country</th>\n",
       "      <th>Language</th>\n",
       "      <th>text_processed</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_tw</th>\n",
       "      <th>stem_lemm_tw</th>\n",
       "      <th>Text_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @rachel_groves: Daily goals: Wake up early....</td>\n",
       "      <td>joscchh</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>en</td>\n",
       "      <td>['daili', 'goal', 'wake', 'earli', 'drink', 'c...</td>\n",
       "      <td>0.194048</td>\n",
       "      <td>0.427381</td>\n",
       "      <td>positive</td>\n",
       "      <td>daily goals wake up early drink coffee work ha...</td>\n",
       "      <td>[daily, goals, wake, up, early, drink, coffee,...</td>\n",
       "      <td>[daili, goal, wake, up, earli, drink, coffe, w...</td>\n",
       "      <td>[daili, goal, wake, earli, drink, coffe, work,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @EurAsiaBridgeEU: good topic - well done.</td>\n",
       "      <td>joscchh</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>en</td>\n",
       "      <td>['good', 'topic', 'well', 'done']</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>positive</td>\n",
       "      <td>good topic  well done</td>\n",
       "      <td>[good, topic, well, done]</td>\n",
       "      <td>[good, topic, well, done]</td>\n",
       "      <td>[good, topic, well, done]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@jpbrice Life is always precious - every singl...</td>\n",
       "      <td>joscchh</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>en</td>\n",
       "      <td>['jpbrice', 'life', 'alway', 'precious', 'ever...</td>\n",
       "      <td>0.272381</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>positive</td>\n",
       "      <td>jpbrice life is always precious  every single ...</td>\n",
       "      <td>[jpbrice, life, is, always, precious, every, s...</td>\n",
       "      <td>[jpbrice, life, is, alway, precious, everi, si...</td>\n",
       "      <td>[jpbrice, life, alway, precious, everi, singl,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@PeterShepherd8 @MYorke27 @ZEReadyOregon Great...</td>\n",
       "      <td>joscchh</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>en</td>\n",
       "      <td>['petershepherd', 'myork', 'zereadyoregon', 'g...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.385417</td>\n",
       "      <td>positive</td>\n",
       "      <td>petershepherd myorke zereadyoregon great  we h...</td>\n",
       "      <td>[petershepherd, myorke, zereadyoregon, great, ...</td>\n",
       "      <td>[petershepherd, myork, zereadyoregon, great, w...</td>\n",
       "      <td>[petershepherd, myork, zereadyoregon, great, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @jpbrice: “You never know how strong you ar...</td>\n",
       "      <td>joscchh</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>en</td>\n",
       "      <td>['never', 'know', 'strong', 'strong', 'onli', ...</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>positive</td>\n",
       "      <td>you never know how strong you are until being ...</td>\n",
       "      <td>[you, never, know, how, strong, you, are, unti...</td>\n",
       "      <td>[you, never, know, how, strong, you, are, unti...</td>\n",
       "      <td>[never, know, strong, strong, onli, choic, quo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           TweetText   Handle  \\\n",
       "0  RT @rachel_groves: Daily goals: Wake up early....  joscchh   \n",
       "1       RT @EurAsiaBridgeEU: good topic - well done.  joscchh   \n",
       "2  @jpbrice Life is always precious - every singl...  joscchh   \n",
       "3  @PeterShepherd8 @MYorke27 @ZEReadyOregon Great...  joscchh   \n",
       "4  RT @jpbrice: “You never know how strong you ar...  joscchh   \n",
       "\n",
       "            Location  Country Language  \\\n",
       "0  Bonn, Deutschland  Germany       en   \n",
       "1  Bonn, Deutschland  Germany       en   \n",
       "2  Bonn, Deutschland  Germany       en   \n",
       "3  Bonn, Deutschland  Germany       en   \n",
       "4  Bonn, Deutschland  Germany       en   \n",
       "\n",
       "                                      text_processed  Polarity  Subjectivity  \\\n",
       "0  ['daili', 'goal', 'wake', 'earli', 'drink', 'c...  0.194048      0.427381   \n",
       "1                  ['good', 'topic', 'well', 'done']  0.700000      0.600000   \n",
       "2  ['jpbrice', 'life', 'alway', 'precious', 'ever...  0.272381      0.609524   \n",
       "3  ['petershepherd', 'myork', 'zereadyoregon', 'g...  0.200000      0.385417   \n",
       "4  ['never', 'know', 'strong', 'strong', 'onli', ...  0.288889      0.822222   \n",
       "\n",
       "  Sentiment                                               text  \\\n",
       "0  positive  daily goals wake up early drink coffee work ha...   \n",
       "1  positive                              good topic  well done   \n",
       "2  positive  jpbrice life is always precious  every single ...   \n",
       "3  positive  petershepherd myorke zereadyoregon great  we h...   \n",
       "4  positive  you never know how strong you are until being ...   \n",
       "\n",
       "                                        tokenized_tw  \\\n",
       "0  [daily, goals, wake, up, early, drink, coffee,...   \n",
       "1                          [good, topic, well, done]   \n",
       "2  [jpbrice, life, is, always, precious, every, s...   \n",
       "3  [petershepherd, myorke, zereadyoregon, great, ...   \n",
       "4  [you, never, know, how, strong, you, are, unti...   \n",
       "\n",
       "                                        stem_lemm_tw  \\\n",
       "0  [daili, goal, wake, up, earli, drink, coffe, w...   \n",
       "1                          [good, topic, well, done]   \n",
       "2  [jpbrice, life, is, alway, precious, everi, si...   \n",
       "3  [petershepherd, myork, zereadyoregon, great, w...   \n",
       "4  [you, never, know, how, strong, you, are, unti...   \n",
       "\n",
       "                                      Text_processed  \n",
       "0  [daili, goal, wake, earli, drink, coffe, work,...  \n",
       "1                          [good, topic, well, done]  \n",
       "2  [jpbrice, life, alway, precious, everi, singl,...  \n",
       "3  [petershepherd, myork, zereadyoregon, great, l...  \n",
       "4  [never, know, strong, strong, onli, choic, quo...  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweets_en.drop(['tokenized_tw', 'stem_lemm_tw', 'text', 'UserDescription', 'CreatedAt', 'ID', 'Follower'], axis=1, inplace=True)\n",
    "tweets_en.rename(columns={'remove_stopwords_tw': 'Text_processed'}, inplace=True)\n",
    "tweets_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TweetText</th>\n",
       "      <th>Handle</th>\n",
       "      <th>Location</th>\n",
       "      <th>Country</th>\n",
       "      <th>Language</th>\n",
       "      <th>text_processed</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_tw</th>\n",
       "      <th>stem_lemm_tw</th>\n",
       "      <th>Text_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4789</th>\n",
       "      <td>RT @z_edian: Unser @snv_berlin Expert:innenwor...</td>\n",
       "      <td>christinacrupp</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>de</td>\n",
       "      <td>['snvberlin', 'expertinnenworkshop', 'staatlic...</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>negative</td>\n",
       "      <td>unser snvberlin expertinnenworkshop zur staatl...</td>\n",
       "      <td>[unser, snvberlin, expertinnenworkshop, zur, s...</td>\n",
       "      <td>[uns, snvberlin, expertinnenworkshop, zur, sta...</td>\n",
       "      <td>[snvberlin, expertinnenworkshop, staatlich, be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4790</th>\n",
       "      <td>RT @michael_adler_: Fing gut an! Bonn braucht ...</td>\n",
       "      <td>joscchh</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>de</td>\n",
       "      <td>['fing', 'gut', 'bonn', 'braucht', 'mindshift'...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>positive</td>\n",
       "      <td>fing gut an bonn braucht einen mindshift von d...</td>\n",
       "      <td>[fing, gut, an, bonn, braucht, einen, mindshif...</td>\n",
       "      <td>[fing, gut, an, bonn, braucht, ein, mindshift,...</td>\n",
       "      <td>[fing, gut, bonn, braucht, mindshift, autostad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4791</th>\n",
       "      <td>Gestern haben @Lichtemomente, @askans, @michae...</td>\n",
       "      <td>joscchh</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>de</td>\n",
       "      <td>['gest', 'lichtemoment', 'askan', 'michaeladl'...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>neutral</td>\n",
       "      <td>gestern haben lichtemomente askans michaeladle...</td>\n",
       "      <td>[gestern, haben, lichtemomente, askans, michae...</td>\n",
       "      <td>[gest, hab, lichtemoment, askan, michaeladl, b...</td>\n",
       "      <td>[gest, lichtemoment, askan, michaeladl, barbar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4792</th>\n",
       "      <td>@VolkerGoebbels Macht Sinn 😏</td>\n",
       "      <td>joscchh</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>de</td>\n",
       "      <td>['volkergoebbel', 'macht', 'sinn']</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>neutral</td>\n",
       "      <td>volkergoebbels macht sinn</td>\n",
       "      <td>[volkergoebbels, macht, sinn]</td>\n",
       "      <td>[volkergoebbel, macht, sinn]</td>\n",
       "      <td>[volkergoebbel, macht, sinn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4793</th>\n",
       "      <td>@jbrunotte Was waren denn deine Beweggründe da...</td>\n",
       "      <td>joscchh</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>de</td>\n",
       "      <td>['jbrunott', 'wa', 'beweggrund', 'dafur', 'spa...</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>positive</td>\n",
       "      <td>jbrunotte was waren denn deine beweggründe daf...</td>\n",
       "      <td>[jbrunotte, was, waren, denn, deine, beweggrün...</td>\n",
       "      <td>[jbrunott, wa, war, denn, dein, beweggrund, da...</td>\n",
       "      <td>[jbrunott, wa, beweggrund, dafur, spannend, wiss]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              TweetText          Handle  \\\n",
       "4789  RT @z_edian: Unser @snv_berlin Expert:innenwor...  christinacrupp   \n",
       "4790  RT @michael_adler_: Fing gut an! Bonn braucht ...         joscchh   \n",
       "4791  Gestern haben @Lichtemomente, @askans, @michae...         joscchh   \n",
       "4792                       @VolkerGoebbels Macht Sinn 😏         joscchh   \n",
       "4793  @jbrunotte Was waren denn deine Beweggründe da...         joscchh   \n",
       "\n",
       "               Location  Country Language  \\\n",
       "4789  Bonn, Deutschland  Germany       de   \n",
       "4790  Bonn, Deutschland  Germany       de   \n",
       "4791  Bonn, Deutschland  Germany       de   \n",
       "4792  Bonn, Deutschland  Germany       de   \n",
       "4793  Bonn, Deutschland  Germany       de   \n",
       "\n",
       "                                         text_processed  Polarity  \\\n",
       "4789  ['snvberlin', 'expertinnenworkshop', 'staatlic... -0.350000   \n",
       "4790  ['fing', 'gut', 'bonn', 'braucht', 'mindshift'...  0.250000   \n",
       "4791  ['gest', 'lichtemoment', 'askan', 'michaeladl'...  0.000000   \n",
       "4792                 ['volkergoebbel', 'macht', 'sinn']  0.000000   \n",
       "4793  ['jbrunott', 'wa', 'beweggrund', 'dafur', 'spa...  0.233333   \n",
       "\n",
       "      Subjectivity Sentiment  \\\n",
       "4789          0.00  negative   \n",
       "4790          0.25  positive   \n",
       "4791          0.00   neutral   \n",
       "4792          0.00   neutral   \n",
       "4793          0.00  positive   \n",
       "\n",
       "                                                   text  \\\n",
       "4789  unser snvberlin expertinnenworkshop zur staatl...   \n",
       "4790  fing gut an bonn braucht einen mindshift von d...   \n",
       "4791  gestern haben lichtemomente askans michaeladle...   \n",
       "4792                         volkergoebbels macht sinn    \n",
       "4793  jbrunotte was waren denn deine beweggründe daf...   \n",
       "\n",
       "                                           tokenized_tw  \\\n",
       "4789  [unser, snvberlin, expertinnenworkshop, zur, s...   \n",
       "4790  [fing, gut, an, bonn, braucht, einen, mindshif...   \n",
       "4791  [gestern, haben, lichtemomente, askans, michae...   \n",
       "4792                      [volkergoebbels, macht, sinn]   \n",
       "4793  [jbrunotte, was, waren, denn, deine, beweggrün...   \n",
       "\n",
       "                                           stem_lemm_tw  \\\n",
       "4789  [uns, snvberlin, expertinnenworkshop, zur, sta...   \n",
       "4790  [fing, gut, an, bonn, braucht, ein, mindshift,...   \n",
       "4791  [gest, hab, lichtemoment, askan, michaeladl, b...   \n",
       "4792                       [volkergoebbel, macht, sinn]   \n",
       "4793  [jbrunott, wa, war, denn, dein, beweggrund, da...   \n",
       "\n",
       "                                         Text_processed  \n",
       "4789  [snvberlin, expertinnenworkshop, staatlich, be...  \n",
       "4790  [fing, gut, bonn, braucht, mindshift, autostad...  \n",
       "4791  [gest, lichtemoment, askan, michaeladl, barbar...  \n",
       "4792                       [volkergoebbel, macht, sinn]  \n",
       "4793  [jbrunott, wa, beweggrund, dafur, spannend, wiss]  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_de.rename(columns={'remove_stopwords_tw': 'Text_processed'}, inplace=True)\n",
    "tweets_de.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_en.drop('text_processed', axis=1, inplace=True)\n",
    "tweets_de.drop('text_processed', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TweetText</th>\n",
       "      <th>Handle</th>\n",
       "      <th>Location</th>\n",
       "      <th>Country</th>\n",
       "      <th>Language</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_tw</th>\n",
       "      <th>stem_lemm_tw</th>\n",
       "      <th>Text_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @rachel_groves: Daily goals: Wake up early....</td>\n",
       "      <td>joscchh</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>en</td>\n",
       "      <td>0.194048</td>\n",
       "      <td>0.427381</td>\n",
       "      <td>positive</td>\n",
       "      <td>daily goals wake up early drink coffee work ha...</td>\n",
       "      <td>[daily, goals, wake, up, early, drink, coffee,...</td>\n",
       "      <td>[daili, goal, wake, up, earli, drink, coffe, w...</td>\n",
       "      <td>[daili, goal, wake, earli, drink, coffe, work,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @EurAsiaBridgeEU: good topic - well done.</td>\n",
       "      <td>joscchh</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>en</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>positive</td>\n",
       "      <td>good topic  well done</td>\n",
       "      <td>[good, topic, well, done]</td>\n",
       "      <td>[good, topic, well, done]</td>\n",
       "      <td>[good, topic, well, done]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@jpbrice Life is always precious - every singl...</td>\n",
       "      <td>joscchh</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>en</td>\n",
       "      <td>0.272381</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>positive</td>\n",
       "      <td>jpbrice life is always precious  every single ...</td>\n",
       "      <td>[jpbrice, life, is, always, precious, every, s...</td>\n",
       "      <td>[jpbrice, life, is, alway, precious, everi, si...</td>\n",
       "      <td>[jpbrice, life, alway, precious, everi, singl,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@PeterShepherd8 @MYorke27 @ZEReadyOregon Great...</td>\n",
       "      <td>joscchh</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>en</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.385417</td>\n",
       "      <td>positive</td>\n",
       "      <td>petershepherd myorke zereadyoregon great  we h...</td>\n",
       "      <td>[petershepherd, myorke, zereadyoregon, great, ...</td>\n",
       "      <td>[petershepherd, myork, zereadyoregon, great, w...</td>\n",
       "      <td>[petershepherd, myork, zereadyoregon, great, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @jpbrice: “You never know how strong you ar...</td>\n",
       "      <td>joscchh</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>en</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>positive</td>\n",
       "      <td>you never know how strong you are until being ...</td>\n",
       "      <td>[you, never, know, how, strong, you, are, unti...</td>\n",
       "      <td>[you, never, know, how, strong, you, are, unti...</td>\n",
       "      <td>[never, know, strong, strong, onli, choic, quo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           TweetText   Handle  \\\n",
       "0  RT @rachel_groves: Daily goals: Wake up early....  joscchh   \n",
       "1       RT @EurAsiaBridgeEU: good topic - well done.  joscchh   \n",
       "2  @jpbrice Life is always precious - every singl...  joscchh   \n",
       "3  @PeterShepherd8 @MYorke27 @ZEReadyOregon Great...  joscchh   \n",
       "4  RT @jpbrice: “You never know how strong you ar...  joscchh   \n",
       "\n",
       "            Location  Country Language  Polarity  Subjectivity Sentiment  \\\n",
       "0  Bonn, Deutschland  Germany       en  0.194048      0.427381  positive   \n",
       "1  Bonn, Deutschland  Germany       en  0.700000      0.600000  positive   \n",
       "2  Bonn, Deutschland  Germany       en  0.272381      0.609524  positive   \n",
       "3  Bonn, Deutschland  Germany       en  0.200000      0.385417  positive   \n",
       "4  Bonn, Deutschland  Germany       en  0.288889      0.822222  positive   \n",
       "\n",
       "                                                text  \\\n",
       "0  daily goals wake up early drink coffee work ha...   \n",
       "1                              good topic  well done   \n",
       "2  jpbrice life is always precious  every single ...   \n",
       "3  petershepherd myorke zereadyoregon great  we h...   \n",
       "4  you never know how strong you are until being ...   \n",
       "\n",
       "                                        tokenized_tw  \\\n",
       "0  [daily, goals, wake, up, early, drink, coffee,...   \n",
       "1                          [good, topic, well, done]   \n",
       "2  [jpbrice, life, is, always, precious, every, s...   \n",
       "3  [petershepherd, myorke, zereadyoregon, great, ...   \n",
       "4  [you, never, know, how, strong, you, are, unti...   \n",
       "\n",
       "                                        stem_lemm_tw  \\\n",
       "0  [daili, goal, wake, up, earli, drink, coffe, w...   \n",
       "1                          [good, topic, well, done]   \n",
       "2  [jpbrice, life, is, alway, precious, everi, si...   \n",
       "3  [petershepherd, myork, zereadyoregon, great, w...   \n",
       "4  [you, never, know, how, strong, you, are, unti...   \n",
       "\n",
       "                                      Text_processed  \n",
       "0  [daili, goal, wake, earli, drink, coffe, work,...  \n",
       "1                          [good, topic, well, done]  \n",
       "2  [jpbrice, life, alway, precious, everi, singl,...  \n",
       "3  [petershepherd, myork, zereadyoregon, great, l...  \n",
       "4  [never, know, strong, strong, onli, choic, quo...  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_analysis_new = pd.concat([tweets_en, tweets_de], ignore_index=True)\n",
    "tweets_analysis_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1:\n",
    "### Prepare the data for continued analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    3043\n",
       "negative     843\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop neutral tweets? How many are left?\n",
    "tweets_analysis_new['Sentiment'].value_counts()\n",
    "tweets = tweets_analysis_new[tweets_analysis_new['Sentiment']!='neutral']\n",
    "tweets['Sentiment'].value_counts()\n",
    "\n",
    "# consider saving this df (excludes neutral tweets) for visualizations in other notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TweetText</th>\n",
       "      <th>Handle</th>\n",
       "      <th>Location</th>\n",
       "      <th>Country</th>\n",
       "      <th>Language</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_tw</th>\n",
       "      <th>stem_lemm_tw</th>\n",
       "      <th>Text_processed</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @rachel_groves: Daily goals: Wake up early....</td>\n",
       "      <td>joscchh</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>en</td>\n",
       "      <td>0.194048</td>\n",
       "      <td>0.427381</td>\n",
       "      <td>positive</td>\n",
       "      <td>daily goals wake up early drink coffee work ha...</td>\n",
       "      <td>[daily, goals, wake, up, early, drink, coffee,...</td>\n",
       "      <td>[daili, goal, wake, up, earli, drink, coffe, w...</td>\n",
       "      <td>[daili, goal, wake, earli, drink, coffe, work,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @EurAsiaBridgeEU: good topic - well done.</td>\n",
       "      <td>joscchh</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>en</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>positive</td>\n",
       "      <td>good topic  well done</td>\n",
       "      <td>[good, topic, well, done]</td>\n",
       "      <td>[good, topic, well, done]</td>\n",
       "      <td>[good, topic, well, done]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@jpbrice Life is always precious - every singl...</td>\n",
       "      <td>joscchh</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>en</td>\n",
       "      <td>0.272381</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>positive</td>\n",
       "      <td>jpbrice life is always precious  every single ...</td>\n",
       "      <td>[jpbrice, life, is, always, precious, every, s...</td>\n",
       "      <td>[jpbrice, life, is, alway, precious, everi, si...</td>\n",
       "      <td>[jpbrice, life, alway, precious, everi, singl,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@PeterShepherd8 @MYorke27 @ZEReadyOregon Great...</td>\n",
       "      <td>joscchh</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>en</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.385417</td>\n",
       "      <td>positive</td>\n",
       "      <td>petershepherd myorke zereadyoregon great  we h...</td>\n",
       "      <td>[petershepherd, myorke, zereadyoregon, great, ...</td>\n",
       "      <td>[petershepherd, myork, zereadyoregon, great, w...</td>\n",
       "      <td>[petershepherd, myork, zereadyoregon, great, l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @jpbrice: “You never know how strong you ar...</td>\n",
       "      <td>joscchh</td>\n",
       "      <td>Bonn, Deutschland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>en</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>positive</td>\n",
       "      <td>you never know how strong you are until being ...</td>\n",
       "      <td>[you, never, know, how, strong, you, are, unti...</td>\n",
       "      <td>[you, never, know, how, strong, you, are, unti...</td>\n",
       "      <td>[never, know, strong, strong, onli, choic, quo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           TweetText   Handle  \\\n",
       "0  RT @rachel_groves: Daily goals: Wake up early....  joscchh   \n",
       "1       RT @EurAsiaBridgeEU: good topic - well done.  joscchh   \n",
       "2  @jpbrice Life is always precious - every singl...  joscchh   \n",
       "3  @PeterShepherd8 @MYorke27 @ZEReadyOregon Great...  joscchh   \n",
       "4  RT @jpbrice: “You never know how strong you ar...  joscchh   \n",
       "\n",
       "            Location  Country Language  Polarity  Subjectivity Sentiment  \\\n",
       "0  Bonn, Deutschland  Germany       en  0.194048      0.427381  positive   \n",
       "1  Bonn, Deutschland  Germany       en  0.700000      0.600000  positive   \n",
       "2  Bonn, Deutschland  Germany       en  0.272381      0.609524  positive   \n",
       "3  Bonn, Deutschland  Germany       en  0.200000      0.385417  positive   \n",
       "4  Bonn, Deutschland  Germany       en  0.288889      0.822222  positive   \n",
       "\n",
       "                                                text  \\\n",
       "0  daily goals wake up early drink coffee work ha...   \n",
       "1                              good topic  well done   \n",
       "2  jpbrice life is always precious  every single ...   \n",
       "3  petershepherd myorke zereadyoregon great  we h...   \n",
       "4  you never know how strong you are until being ...   \n",
       "\n",
       "                                        tokenized_tw  \\\n",
       "0  [daily, goals, wake, up, early, drink, coffee,...   \n",
       "1                          [good, topic, well, done]   \n",
       "2  [jpbrice, life, is, always, precious, every, s...   \n",
       "3  [petershepherd, myorke, zereadyoregon, great, ...   \n",
       "4  [you, never, know, how, strong, you, are, unti...   \n",
       "\n",
       "                                        stem_lemm_tw  \\\n",
       "0  [daili, goal, wake, up, earli, drink, coffe, w...   \n",
       "1                          [good, topic, well, done]   \n",
       "2  [jpbrice, life, is, alway, precious, everi, si...   \n",
       "3  [petershepherd, myork, zereadyoregon, great, w...   \n",
       "4  [you, never, know, how, strong, you, are, unti...   \n",
       "\n",
       "                                      Text_processed  target  \n",
       "0  [daili, goal, wake, earli, drink, coffe, work,...       1  \n",
       "1                          [good, topic, well, done]       1  \n",
       "2  [jpbrice, life, alway, precious, everi, singl,...       1  \n",
       "3  [petershepherd, myork, zereadyoregon, great, l...       1  \n",
       "4  [never, know, strong, strong, onli, choic, quo...       1  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add ['target'] column with 0 if negative, 1 is positive\n",
    "\n",
    "tweets['target'] = np.where((tweets['Sentiment']=='negative'), 0, 1)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3043\n",
       "0     843\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer=lambda x: x) # adding this argument here will allow you to apply it to the entire df in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(tweets['Text_processed']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2:\n",
    "### Split into train and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, tweets['target'], test_size=0.33, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict whether the tweet is positive\n",
    "predictions = clf.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8347622759158223"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score:  0.8351464435146444\n",
      "Recall score:  0.9851924975320829\n",
      "F1 score:  0.9039855072463769\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision score: \", precision_score(y_test, predictions))\n",
    "print(\"Recall score: \", recall_score(y_test, predictions))\n",
    "print(\"F1 score: \", f1_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 73 197]\n",
      " [ 15 998]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, predictions)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11aa6d670>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot=True,\n",
    "           fmt='.2%', cmap='Blues')\n",
    "\n",
    "# This cant be right... the true predictions should fall along the diagonal axis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3:\n",
    "### Predict probability of positive tweet by `['Country']`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
